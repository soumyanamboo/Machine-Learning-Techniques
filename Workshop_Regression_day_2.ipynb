{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RQLKkM58BiF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data \n",
        "\n",
        "$\\mathbf{X}$ is a feature matrix corresponding to $n$ training examples, each represented with $d$ features and has shape $d \\times n$.  \n",
        "\\begin{equation}\n",
        "  \\mathbf{X}_{d \\times n} = \\begin{bmatrix}\n",
        "       | & | &\\dots & |\\\\\n",
        "       \\mathbf{x}_{1}&  \n",
        "       \\mathbf{x}_{2} & \n",
        "      \\dots &\n",
        "       \\mathbf{x}_{n}  \\\\\n",
        "        | & | &\\dots & |\\\\\n",
        "    \\end{bmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "* here each $\\mathbf{x}_i \\in \\mathbb{R}^d$\n",
        "\n",
        "*  $\\mathbf{y}$ is a label vector of shape $n \\times 1$.  \n",
        "\\begin{equation}\n",
        "    \\mathbf{y} = \\begin{bmatrix}\n",
        "        y_{1}  \\\\\n",
        "        y_{2} \\\\\n",
        "        \\vdots  \\\\\n",
        "        y_{n} \\\\\n",
        "      \\end{bmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "*  The $i$-th entry in this vector, $\\mathbf{y}[i]$ gives label for $i$-th example, which is denoted by $y_{i} \\in \\mathbb{R}$."
      ],
      "metadata": {
        "id": "FAlr8M-Z8Zor"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training data"
      ],
      "metadata": {
        "id": "5xKGE4b6hTq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Training Data \n",
        "rng = np.random.default_rng(seed = 101)\n",
        "X_train = (np.arange(-2, 2, 0.01).reshape(1, -1))\n",
        "y_train = (2*X_train**3+3*X_train**2 +4 + rng.normal(0, 1, X_train.shape[1]).reshape(1, -1)).T\n"
      ],
      "metadata": {
        "id": "GbDDcyLKiGsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test data"
      ],
      "metadata": {
        "id": "ffSQPM_4BQya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Test data\n",
        "rng = np.random.default_rng(seed = 102)\n",
        "X_test = np.arange(-2, 2, 0.02).reshape(1, -1)\n",
        "y_test = (2*X_test**3+3*X_test**2 +4 + rng.normal(0, 2, X_test.shape[1]).reshape(1, -1)).T"
      ],
      "metadata": {
        "id": "kACpsUAh8Xpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model\n",
        "\n",
        "Linear regression model for the dataset ${\\mathbb{x}, y}$ is given as\n",
        "$$h_w(\\mathbb{x}) = w_1x^{1}+w_2x^{2}+...+w_dx^{d} =  \\mathbf{x}^Tw\n",
        "$$\n",
        "\n",
        "where $x^{i}$ is the $i^{th}$ feature of the data point $\\mathbf{x}$ and $w = [w_1, w_2, ...w_d]^T$ is the weight vector.\n",
        "\n",
        "\n",
        "Notice that above model always pass through the origin but for a given dataset, best fit model need not pass through the origin. To tackle this issue, we add an intercept $w_0$ in the model and set the corresponding featrue $x^{0}$ to $1$. That is \n",
        "\n",
        "$$h_w(\\mathbb{x}) =w_0x^{0}+ w_1x^{1}+w_2x^{2}+...+w_dx^{n} =  \\mathbf{x}^Tw\n",
        "$$\n",
        "\n",
        "We call $x^{0}$ the dummy feature and set its value to 1 for each examples. Now $w$ is of shape $(d+1, 1)$ and $\\mathbf{X}$ is of shape $(d+1, n)$ where the first row of $\\mathbf{X}$ has entries as 1.\n"
      ],
      "metadata": {
        "id": "EmK77bhCkKI-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add dummy feature"
      ],
      "metadata": {
        "id": "A156drNBk37x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss function\n",
        "\n",
        "The total loss is the sum of square of errors between actual and predicted labels for each training point.  \n",
        "\n",
        "The error for $i$-th training point is calculated as follows:\n",
        "\\begin{eqnarray}\n",
        "e^{(i)} &=& (\\mathrm{{actual\\ label} - {predicted\\ label}})^2 \\\\\n",
        "        &=& \\left ({y_{i}} -\n",
        "        {h_{\\mathbf{w}}(\\mathbf{x}_{i})} \\right)^2 \\\\\n",
        "        &=& \\left ({y_{i}} -\n",
        "        {\\mathbf{w}^T \\mathbf{x}_{i}} \\right)^2\n",
        "\\end{eqnarray}\n",
        "\n",
        " \n",
        "\n",
        "The total loss $L(\\mathbf{w})$ is sum of errors at each training point:\n",
        "\\begin{equation}\n",
        "L(\\mathbf{w}) = \\sum_{i=1}^{n} e^{(i)} \n",
        "\\end{equation}\n",
        "\n",
        "We divide this by $\\frac{1}{2}$ for mathematical convenience in later use:\n",
        "\n",
        "\n",
        "\\begin{eqnarray}\n",
        "  L(\\mathbf{w}) &=& \\frac{1}{2} \\sum_{i=1}^{n} e^{(i)} \\\\ &=&  \\frac{1}{2} \\left({{\\mathbf{X}^T \\mathbf{w}}} - \\mathbf{y} \\right)^T \\left({{\\mathbf{X}^T \\mathbf{w}}} - \\mathbf{y} \\right)) \n",
        "\\end{eqnarray}\n"
      ],
      "metadata": {
        "id": "Wp1v00V9lBMd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimization\n",
        "\n"
      ],
      "metadata": {
        "id": "MQtN8F2j9r42"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Normal equation\n",
        "\n",
        "Let's set $\\dfrac{\\partial L(\\mathbf{w})}{\\partial \\mathbf{w}}$ to 0 and solve for $\\mathbf{w}$:\n",
        "\\begin{eqnarray}\n",
        "\\frac{\\partial L(\\mathbf{w})}{\\partial \\mathbf{w}} &=& \\mathbf{X} \\mathbf{X}^T \\mathbf{w} - \\mathbf{X} \\mathbf{y} = 0 \\\\\n",
        "\\mathbf{X}^T \\mathbf{X} \\mathbf{w} &=& \\mathbf{X}^T \\mathbf{Y} \\\\\n",
        "\\mathbf{w} &=& \\left( \\mathbf{X} \\mathbf{X}^T \\right)^{-1} \\mathbf{X} \\mathbf{y}\n",
        "\\end{eqnarray}\n",
        "\n"
      ],
      "metadata": {
        "id": "GvxZvoJO-OIn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Find weight vector and prediction for training and test dataset"
      ],
      "metadata": {
        "id": "gwcJYj2Ml71z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### weight "
      ],
      "metadata": {
        "id": "UiO64nmAeSkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Prediction"
      ],
      "metadata": {
        "id": "b_KErNi9eXTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot weight vector"
      ],
      "metadata": {
        "id": "ohC5eG-GETPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Plot weight vector"
      ],
      "metadata": {
        "id": "PEUE5Fhvea3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation \n",
        "\n",
        "##### RMSE $$ \\sqrt{\\dfrac{1}{n}\\sum\\limits_{i=1}^{n} (y_i- \\hat{y}_i)^2}\n",
        "$$\n",
        "\n"
      ],
      "metadata": {
        "id": "Sk1KvxpCn1Xr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Training error and test error"
      ],
      "metadata": {
        "id": "zVMo6BF_ediw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Gradient Descent"
      ],
      "metadata": {
        "id": "r8MYjq1qHnM1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Gradient of a function $f$ at a point $x$ gives the direction of maximum change in $f$ at the point $x$.\n",
        "\n",
        "---\n",
        "\n",
        "We start at a random point $\\mathbf{w}$ at the loss function $L(\\mathbf{w})$.\n",
        "\n",
        "\n",
        "We basically need an update rule in gradient descent, which is as follows:\n",
        "\n",
        "\\begin{eqnarray}\n",
        "\\mathbf{w}_{k+1} &:=& \\mathbf{w}_k - \\alpha{\\frac{\\partial L(\\mathbf{w})}{\\partial \\mathbf{w}}} \\\\\n",
        "   &:=& \\mathbf{w}_k - \\alpha{\\left(\\mathbf{X} \\mathbf{X}^T \\mathbf{w}_{k} - \\mathbf{X} \\mathbf{y}\\right)} \\\\\n",
        "\\end{eqnarray}\n",
        "\n",
        "Note that this is the vectorized implementation.  It will make sure that all the parameters are updated in one go through this.\n"
      ],
      "metadata": {
        "id": "SxdpExPLHw5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Gradient descent"
      ],
      "metadata": {
        "id": "aJ08wiSLeiMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Number of iterations vs loss (for fixed alpha)"
      ],
      "metadata": {
        "id": "kQmmqovFoKkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Plot nunber of iterations vs loss"
      ],
      "metadata": {
        "id": "5rGjFal8ekdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###9 SGD:\n",
        "\n"
      ],
      "metadata": {
        "id": "PDmCReoUofgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### SGD"
      ],
      "metadata": {
        "id": "BTz4yLWuwwb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NxONb5vnwvp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kernel Regression\n",
        "\n"
      ],
      "metadata": {
        "id": "Nco8j8s1VOFa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The optimal weight vector can be written as linear combination of data points.\n",
        "\n",
        "---\n",
        "\n",
        "Let $\\phi$ be the trainsformation mapping that trasforms the $d$-dimensional data points to $D$-dimension.\n",
        "\n",
        "$$\\phi: \\mathbb{R}^d \\rightarrow \\mathbb{R}^D$$\n",
        "\n",
        "Then\n",
        "\n",
        "$$\\mathbf{w} = \\phi(\\mathbf{X})\\alpha$$\n",
        "\n",
        "where,\n",
        "\n",
        "* $\\mathbf{X}$ = Feature matrix of shape $(d, n)$\n",
        "* $\\alpha = [\\alpha_1, \\alpha_2, ..., \\alpha_n]^T \\in \\mathbb{R}^n$\n",
        "\n"
      ],
      "metadata": {
        "id": "-lEj5azOBtAd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here \n",
        "\n",
        "$$\\alpha = K^{-1}y$$\n",
        "\n",
        "where,\n",
        "\\begin{equation}\n",
        " K_{n \\times n} = \\begin{bmatrix}\n",
        "       k(\\mathbf{x}_1,\\mathbf{x}_1) & k(\\mathbf{x}_1,\\mathbf{x}_2) &\\dots & k(\\mathbf{x}_1,\\mathbf{x}_n)\\\\\n",
        "        k(\\mathbf{x}_2,\\mathbf{x}_1) & k(\\mathbf{x}_2,\\mathbf{x}_2) &\\dots & k(\\mathbf{x}_2,\\mathbf{x}_n)\\\\\n",
        "        \\vdots & \\vdots & \\dots & \\vdots\\\\\n",
        "       k(\\mathbf{x}_n,\\mathbf{x}_1) & k(\\mathbf{x}_n,\\mathbf{x}_2) &\\dots & k(\\mathbf{x}_n,\\mathbf{x}_n)\\\\\n",
        "    \\end{bmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "Here, $k$ is the kernel function\n",
        "\n",
        "$$k: \\mathbb{R}^d \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}\\\\ k(x_1, x_2) = \\phi(x_1)^T\\phi(x_2)\n",
        "$$"
      ],
      "metadata": {
        "id": "kp20Xt_GDT3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### K matrix and coefficient"
      ],
      "metadata": {
        "id": "nMWfCoejerpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction"
      ],
      "metadata": {
        "id": "__g386ueGBoH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a test point $\\mathbf{x}_t$, prediction is given by\n",
        "\n",
        "$$\\sum\\limits_{i=1}^{n}k(\\mathbf{x}_i, \\mathbf{x}_t)\\alpha_i$$\n",
        "\n"
      ],
      "metadata": {
        "id": "WEkJCpdVGFQ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Polynomial kernel of degree $p$:\n",
        "\n",
        "\n",
        "$k(\\mathbf{x}_1, \\mathbf{x}_2) = (\\mathbf{x}_1^T\\mathbf{x}_2+1)^p$"
      ],
      "metadata": {
        "id": "AGPoE7q3Hx-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Polynomial Kernel"
      ],
      "metadata": {
        "id": "EhpPUAOuLNkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### For prediction\n",
        "\n",
        "Let the training data matrix with $n$ examples is given by\n",
        " \\begin{equation}\n",
        "  \\mathbf{X}_{train} = \\begin{bmatrix}\n",
        "       | & | &\\dots & |\\\\\n",
        "       \\mathbf{x}_{1}&  \n",
        "       \\mathbf{x}_{2} & \n",
        "      \\dots &\n",
        "       \\mathbf{x}_{n}  \\\\\n",
        "        | & | &\\dots & |\\\\\n",
        "    \\end{bmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "and the test data matrix with $m$ examples is given by\n",
        " \\begin{equation}\n",
        "  \\mathbf{X}_{test} = \\begin{bmatrix}\n",
        "       | & | &\\dots & |\\\\\n",
        "       \\mathbf{x}_{t1}&  \n",
        "       \\mathbf{x}_{t2} & \n",
        "      \\dots &\n",
        "       \\mathbf{x}_{tm}  \\\\\n",
        "        | & | &\\dots & |\\\\\n",
        "    \\end{bmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "----\n",
        "\n",
        "Then \n",
        "\n",
        "\\begin{equation}\n",
        "  \\mathbf{X}_{train}^T \\mathbf{X}_{test}  = \\begin{bmatrix}\n",
        "       \\mathbf{x}_1^T\\mathbf{x}_{t1} &  \\mathbf{x}_1^T\\mathbf{x}_{t2}  &\\dots &  \\mathbf{x}_1^T\\mathbf{x}_{tm} \\\\\n",
        "      \\mathbf{x}_2^T\\mathbf{x}_{t1} &  \\mathbf{x}_2^T\\mathbf{x}_{t2}  &\\dots &  \\mathbf{x}_2^T\\mathbf{x}_{tm} \\\\\n",
        "        \\vdots & \\vdots &\\dots & \\vdots\\\\\n",
        "         \\mathbf{x}_n^T\\mathbf{x}_{t1} &  \\mathbf{x}_n^T\\mathbf{x}_{t2}  &\\dots &  \\mathbf{x}_n^T\\mathbf{x}_{tm} \\\\\n",
        "    \\end{bmatrix}\n",
        "\\end{equation}\n"
      ],
      "metadata": {
        "id": "TkjwkpKpOXxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Prediction"
      ],
      "metadata": {
        "id": "72M-Xoiwe8Ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### degree vs loss"
      ],
      "metadata": {
        "id": "5OnaPAgPU29n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Plot degree vs loss"
      ],
      "metadata": {
        "id": "70cn-98Se9lH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}