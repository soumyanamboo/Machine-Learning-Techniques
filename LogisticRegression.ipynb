{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+XX+lRKtdMvznxoFbzCkY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soumyanamboo/Machine-Learning-Techniques/blob/main/LogisticRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Logistic Regression"
      ],
      "metadata": {
        "id": "VWi4MGi9HwFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import libraries\n",
        "from IPython.display import display, Math, Latex\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "np.random.seed(1234)"
      ],
      "metadata": {
        "id": "VZEW6wc6HvgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Linear combination of features $z = w^T x$\n",
        "2. Apply Sigmoid or logistic activation on the linear combinations (z) to obtain the probability.   \n",
        "$$Pr(y=1|x) = sigmoid(z) = \\frac{1}{1+e^{-z}}\n",
        "$$   \n",
        "By vectorisint it, $$z_{(n,1)} = X_{(n,m)} w_{(m,1)}    $$"
      ],
      "metadata": {
        "id": "-RZC4PYlIbcq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVkrKsZhHr2J"
      },
      "outputs": [],
      "source": [
        "def linear_combination(X:np.ndarray, w:np.ndarray):\n",
        "  ''' linear combination z = Xw'''\n",
        "  z = X@w\n",
        "  return z\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sigmoid function applied on z,  \n",
        "$$ P(y=1|X) = sigmoid(z_{(n,1)} $$"
      ],
      "metadata": {
        "id": "vw_vcr_6Lsa5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z:np.ndarray):\n",
        "  ''' Calculates sigmoid of linear combinations z'''\n",
        "  s = 1/(1+np.exp(-z))\n",
        "  return s"
      ],
      "metadata": {
        "id": "6JsM-NPTL92g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply a prediction or inference function on the activations to obtain a class label. If activation or probability > threshold, then label the sample with class 1, or 0. "
      ],
      "metadata": {
        "id": "qCd88M1kMTIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X:np.ndarray, w:np.ndarray, threshold:float):\n",
        "  ''' Predicts class label for sample\n",
        "      if sigmoid > threshols, label = 1. Otherwise label=0\n",
        "  '''\n",
        "  return np.where(sigmoid(linear_combination(X,w)) > threshold, 1, 0)"
      ],
      "metadata": {
        "id": "8Oa54h29Msk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss function**  \n",
        "Binary Cross Entropy:  \n",
        "$$ BCE = BCE on training samples + λ regularisation penalty$$\n",
        "1. if no regularisation, $λ = 0, BCE = BCE$   \n",
        "2. With L2 regularisation, the loss function is:  \n",
        "$$ J(w) = -(Σ_{i=1}^n y^i log(sigmoid(w^Tx^i)) + (1 - y^i) log(1 - sigmoid(w^Tx^i)))  + λ ||w||^2 $$   \n",
        "3. With L1 regularization, the loss function is:   \n",
        "$$J(w) = -(Σ_{i=1}^n y^i log(sigmoid(w^Tx^i)) + (1 - y^i) log(1 - sigmoid(w^Tx^i)))  + λ ||w|| $$  \n",
        "\n",
        "Loss function in vectorised form is: \n",
        "$$ e = (y log(sigmoid(Xw)) + (1-y)log(1-sigmoid(Xw))$$\n",
        "$$ Loss, J(w) = -1^T_{(1,n)} e_{(n,1)} $$\n",
        "Adding L2 penality, $$ J(w) = -1^Te + λ w^Tw $$\n",
        "* Adding L1 penality, $$ J(w) = -1^Te + λ1^T|w| $$\n",
        "* Set the regularization rate L1 or L2, whichever not required to 0.   \n",
        "* If we set the regularization rate such that their sum is 1, we get **elastic net regularization**.\n"
      ],
      "metadata": {
        "id": "yDxLDL0FNXXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(y, sigmoid_vector, weight_vector, l1_reg_rate, l2_reg_rate):\n",
        "  bce = -1 * (np.sum(y * np.log(sigmoid_vector)  + (1-y)* np.log(1-sigmoid_vector)))\n",
        "  l2_reg = l2_reg_rate * np.transpose(weight_vector)@weight_vector\n",
        "  l1_reg = l1_reg_rate * np.sum(np.abs(weight_vector))\n",
        "  loss_value = bce + l2_reg + l1_reg\n",
        "  return loss_value"
      ],
      "metadata": {
        "id": "GSMgWEvlQxne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optimization:**\n",
        "1. Calculate gradient of loss function\n",
        "2. Scale the gradient with learnign rate and use it for updating the weight vector.\n",
        "Gradient of Loss Function:  \n",
        "$$ \\frac{dJ(w)}{dw} = X^T (sigmoid(Xw)-y) + λ w  $$\n"
      ],
      "metadata": {
        "id": "GAhXaodQSjwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_gradient(X:np.ndarray, y:np.ndarray, w:np.ndarray, reg_rate:float):\n",
        "  ''' Calculate the gradient of loss function w.r.t the weight vector on training dataset\n",
        "      gradient is calculated as np.transpose(X)(sigmoid(Xw) - y) + reg_rate * w '''\n",
        "  \n",
        "  grad = np.transpose(X) @ (sigmoid(linear_combination(X,w))) - y + reg_rate * w\n",
        "  return grad"
      ],
      "metadata": {
        "id": "fYTzqSy1TXd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Logistic_Regression(object):\n",
        "  ''' Logistic regression model : y = sigmoid (X@w)'''\n",
        "\n",
        "  def set_weight_vector(self,w):\n",
        "    self.w = w\n",
        "  \n",
        "  def linear_combination(self, X:np.ndarray):\n",
        "    return X @ self.w\n",
        "  \n",
        "  def sigmoid(self, z:np.ndarray):\n",
        "    return (1/(1 + np.exp(-z)))\n",
        "  \n",
        "  def activation(self, X:np.ndarray):\n",
        "    '''calculates sigmoid activation for logistic regression as act = sigmoid(Xw)'''\n",
        "\n",
        "    return self.sigmoid(self.linear_combination(X))\n",
        "  \n",
        "  def predict(self, X:np.ndarray, threshold:float = 0.5):\n",
        "    return (self.activation(X) > threshold).astype(int)\n",
        "  \n",
        "  def loss(self,X:np.ndarray,y:np.ndarray, reg_rate:float):\n",
        "    predicted_prob = self.activation(X)\n",
        "    loss = (-1 * np.sum(y * np.log(predicted_prob) + (1-y)*np.log(1 - predicted_prob))) + \n",
        "          reg_rate * (np.transpose(self.w) @ self.w)\n",
        "    return loss\n",
        "  \n",
        "  def calculate_gradient(self, X:np.ndarray, y:np.ndarray,reg_rate:float):\n",
        "    grad = np.transpose(X) @ (self.activation(X) - y) + reg_rate * self.w\n",
        "    return grad\n",
        "  \n",
        "  def update_weight(self, grad:np.ndarray, lr:float):\n",
        "    return (self.w - lr * grad)\n",
        "  \n",
        "  def gd(self, X:np.ndarray, y:np.ndarray, num_epochs:int, lr:float, reg_rate:float):\n",
        "    self.w = np.zeros(X.shape[1])\n",
        "    self.w_all = []\n",
        "    self.err_all = []\n",
        "    for i in np.arange(0, num_epochs):\n",
        "      dJdw = self.calculate_gradient(X, y, reg_rate)\n",
        "      self.w_all.append(self.w)\n",
        "      self.err_all.append(self.loss(X, y, reg_rate))\n",
        "      self.w = self.update_weight(dJdw, lr)\n",
        "    return self.w"
      ],
      "metadata": {
        "id": "BYUa8j00TXTo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}