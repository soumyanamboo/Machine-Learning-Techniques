{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYv/xjmBqvZqAcs64l5ZTn"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4gPp78rCx_I"
      },
      "outputs": [],
      "source": [
        "# [import libraries]\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Linear Regression Class Defenition\n",
        "class LinReg():\n",
        "  ''' Linear Regression Model class definition\n",
        "      y = X@w\n",
        "      X: feature matrix\n",
        "      w: weight vector\n",
        "      y: label vector\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    self.t0 = 20\n",
        "    self.t1 = 100\n",
        "  \n",
        "  def predict(self, X:np.ndarray):\n",
        "    ''' Args:\n",
        "          X: feature matrix\n",
        "        Returns:\n",
        "          y: label vector predicted by the given model\n",
        "    '''\n",
        "    y = X@self.w\n",
        "    return y\n",
        "  \n",
        "  def loss(self, X:np.ndarray, y:np.ndarray, reg_rate:float):\n",
        "    ''' Calculates loss for a model based on known labels.\n",
        "        Args:\n",
        "          X: feature matrix\n",
        "          y: label vector\n",
        "          reg_rate: regularization rate\n",
        "        Returns:\n",
        "          Loss \n",
        "    '''\n",
        "    e = y - self.predict(X)\n",
        "    return (1/2) * np.transpose(e)@e\n",
        "    # return (1/2) * np.transpose(e)@e + (reg_rate/2)*np.transpose(self.w)@self.w\n",
        "  \n",
        "  def rmse(self, X:np.ndarray, y:np.ndarray):\n",
        "    ''' Calculates root mean squared error of prediction w.r.t actual label\n",
        "        Args:\n",
        "          X: feature matrix\n",
        "          y: label vector\n",
        "        Returns:\n",
        "          Loss\n",
        "    '''\n",
        "    return np.sqrt((2/X.shape[0])* self.loss(X,y,0))\n",
        "  \n",
        "  def fit(self, X:np.ndarray, y:np.ndarray, reg_rate:float):\n",
        "    ''' Estimate parameters of linear regression model w.r.t known labels.\n",
        "        Args:\n",
        "          X: feature matrix\n",
        "          y: label vector\n",
        "          reg_rate: rate of regression\n",
        "        Returns:\n",
        "          weight vector\n",
        "    '''\n",
        "    self.w = np.zeros(X.shape[1])\n",
        "    eye = np.eye(np.size(X,1))\n",
        "    self.w = np.linalg.solve(reg_rate*eye + X.T@X, X.T@y)\n",
        "    return self.w\n",
        "  \n",
        "  def calculate_gradient(self, X:np.ndarray, y:np.ndarray, reg_rate:float):\n",
        "    ''' Calculates the gradient of loss function w.r.t weight vector\n",
        "        Args:\n",
        "          X: feature matrix\n",
        "          y: label vector\n",
        "          reg_rate: rate of regression\n",
        "        Returns:\n",
        "          gradient vector\n",
        "    '''\n",
        "    grad = np.transpose(X) @ (self.predict(X) - y) + reg_rate * self.w\n",
        "    return grad\n",
        "  \n",
        "def update_weights(self, grad:np.ndarray, lr:float):\n",
        "  ''' updates the weights based on the gradient of loss function\n",
        "      w_new = w_old - lr * grad\n",
        "      Args:\n",
        "        grad: gradient of loss w.r.t w\n",
        "        lr: learning rate\n",
        "      Returns:\n",
        "      updated weight vector\n",
        "  '''\n",
        "  w_new = self.w - lr * grad\n",
        "  return w_new\n",
        "\n",
        "def learning_schedule(self, t):\n",
        "  return (self.t0/(t + self.t1))\n",
        "\n",
        "def gd(self, X:np.ndarray, y:np.ndarray, num_epochs:int, lr:float,reg_rate:float):\n",
        "  ''' Estimates parameter of linear regression model using gradient descent\n",
        "      Args:\n",
        "        X: feature matrix\n",
        "        y: label vector\n",
        "        num_epochs: number of iterations\n",
        "        lr: learning rate\n",
        "        reg_rate: rate of regression\n",
        "      Returns:\n",
        "        weight vector: final weight vector\n",
        "  '''\n",
        "  self.w = np.zeros(X.shape[1])\n",
        "  self.w_all = []\n",
        "  self.err_all = []\n",
        "  for i in np.arange(0, num_epochs):\n",
        "    \n",
        "    self.w_all.append(self.w)\n",
        "    self.err_all.append(self.loss(X,y,0))\n",
        "    \n",
        "    grad = self.calculate_gradient(X,y,reg_rate)\n",
        "    self.w = self.update_weights(grad,lr)\n",
        "  return self.w\n",
        "\n",
        "def mbgd(self, X:np.ndarray, y:np.ndarray, num_epochs:int, batch_size:int, reg_rate:float):\n",
        "  ''' Estimates parameter of linear regression model using MBGD\n",
        "      Args:\n",
        "        X: feature matrix\n",
        "        y: label vector\n",
        "        num_epochs: number of iterations\n",
        "        batch_size: number of examples in a batch\n",
        "        reg_rate: rate of regression\n",
        "      Returns:\n",
        "        weight vector: final weight vector\n",
        "  '''\n",
        "  self.w = np.zeros(X.shape[1])\n",
        "  self.w_all = []\n",
        "  self.err_all = []\n",
        "  mini_batch_id = 0\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    shuffled_indices = np.random.permutation(X.shape[0])\n",
        "    X_shuffled = X[shuffled_indices]\n",
        "    y_shuffled = y[shuffled_indices]\n",
        "    for i in range(0, X.shape[0],batch_size):\n",
        "      mini_batch_id += 1\n",
        "      xi = X_shuffled[i:i+batch_size]\n",
        "      yi = y_shuffled[i:i+batch_size]\n",
        "      self.w_all.append(self.w)\n",
        "      self.err_all.append(self.loss(xi,yi,0))\n",
        "\n",
        "      grad = (2/batch_size) * self.calculate_gradient(X,y,reg_rate)\n",
        "      lr = self.learning_schedule(mini_batch_id)\n",
        "      self.w = self.update_weights(grad,lr)\n",
        "  return self.w\n",
        "\n",
        "def sgd(self, X:np.ndarray, y:np.ndarray, num_epochs:int, reg_rate:float):\n",
        "  ''' Estimates parameter of linear regression model using Stochastic GD\n",
        "      Args:\n",
        "        X: feature matrix\n",
        "        y: label vector\n",
        "        num_epochs: number of iterations\n",
        "        reg_rate: rate of regression\n",
        "      Returns:\n",
        "        weight vector: final weight vector\n",
        "  '''\n",
        "  self.w = np.zeros(X.shape[1])\n",
        "  self.w_all = []\n",
        "  self.err_all = []\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    for i in range(0, X.shape[0]):\n",
        "      random_index = np.random.randint(X.shape[0])\n",
        "      xi = X[random_index:random_index+1]\n",
        "      yi = y[random_index:random_index+1]\n",
        "\n",
        "      self.w_all.append(self.w)\n",
        "      self.err_all.append(self.loss(xi,yi,0))\n",
        "\n",
        "      grad = 2 * self.calculate_gradient(X,y,reg_rate)\n",
        "      lr = self.learning_schedule(epoch *X.shape[0] + i)\n",
        "      self.w = self.update_weights(grad,lr)\n",
        "  return self.w\n"
      ],
      "metadata": {
        "id": "ZA3WuPYCC2P7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}